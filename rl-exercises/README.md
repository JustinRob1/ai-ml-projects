# Reinforcement Learning Exercises

This repository contains a collection of Jupyter notebooks which demonstrates practical exercises in reinforcement learning (RL). Each notebook focuses on a core RL concept or algorithm, and documents my implementation, experimentation, and analysis.

## Purpose

The purpose of this repository is to:

- Practical experience with key RL algorithms and concepts.
- Demonstrate implementation of both tabular and function approximation methods.
- Highlight experimentation and analysis of RL algorithms in classic environments.

## Notebooks Overview

### 1. `bandits.ipynb`
The multi-armed bandit problem is implemented and analyzed, focusing on the exploration-exploitation trade-off. The notebook covers greedy and epsilon-greedy agents, and compares different step-size strategies for value estimation.

### 2. `dp.ipynb`
Dynamic programming methods are applied to solve Markov Decision Processes (MDPs), including policy evaluation, policy iteration, and value iteration. These methods are used to solve a parking management problem and find optimal policies.

### 3. `policy-eval.ipynb`
Temporal Difference (TD(0)) learning is used for model-free prediction. A TD(0) agent is implemented to estimate state values under a fixed policy in the Cliff Walking gridworld, gaining experience with bootstrapping and sample-based updates.

### 4. `func-approx.ipynb`
Function approximation in RL is explored using tile coding with the Sarsa algorithm. A Sarsa agent with tile coding is implemented to solve the continuous-state Mountain Car environment and compare different tile coding configurations.

### 5. `dyna-q.ipynb`
Model-based RL is implemented through the Dyna-Q and Dyna-Q+ algorithms, combining direct RL with planning from a learned model. Their performance is evaluated in a maze environment that changes over time.

### 6. `actor-critic.ipynb`
Advanced RL with function approximation and policy gradient methods is explored. An average-reward softmax actor-critic agent is implemented to solve the Pendulum Swing-Up task, using tile coding for state representation and softmax policies for action selection.

### 7. `deep-q-learning.ipynb`
A Deep Q-Network (DQN) agent is implemented to solve the CartPole environment, incorporating experience replay and target networks to stabilize training.

### 8. `semi-grad-td.ipynb`
Semi-gradient TD methods for value function approximation are explored, implementing a semi-gradient TD(0) agent using linear function approximation to estimate state values in the Mountain Car environment.

### 9. `td.ipynb`
Various Temporal Difference learning methods, including Sarsa, Q-learning, and Expected Sarsa, are implemented and compared in the Cliff Walking environment.